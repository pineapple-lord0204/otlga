import os

# å¿…é¡»åœ¨å¯¼å…¥ transformers ä¹‹å‰è®¾ç½®é•œåƒçŽ¯å¢ƒå˜é‡
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel
import logging

# å¯¼å…¥æœ¬åœ°å®šä¹‰çš„ ViT
from vit_custom import get_ViT, create_eva_vit_g

# ==========================================
# 1. å±€éƒ¨-å…¨å±€æ³¨æ„åŠ›æ¨¡å— (LGA)
# ==========================================
class LocalGlobalAttention(nn.Module):
    def __init__(self, d, num_heads=8, dropout=0.2):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d, num_heads, dropout=dropout, batch_first=True)
        self.norm = nn.LayerNorm(d)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, local_feats, global_feat):
        """
        local_feats: [B, n, d]
        global_feat: [B, d]
        """
        # å°†å…¨å±€ç‰¹å¾æ‹¼æŽ¥åˆ°å±€éƒ¨åºåˆ—ä¸­è¿›è¡Œäº¤äº’
        query = torch.cat([global_feat.unsqueeze(1), local_feats], dim=1)
        attn_out, _ = self.self_attn(query, query, query)
        attn_out = self.dropout(attn_out)
        out = self.norm(query + attn_out)
        
        # åˆ†ç¦»å¢žå¼ºåŽçš„ç‰¹å¾
        v_enhanced = out[:, 0, :]
        V_enhanced = out[:, 1:, :]
        return V_enhanced, v_enhanced

# ==========================================
# 2. ç†µæ­£åˆ™æœ€ä¼˜ä¼ è¾“ (OT)
# ==========================================
class EntropicOT(nn.Module):
    def __init__(self, epsilon=0.05, max_iter=50):
        super().__init__()
        self.epsilon = epsilon
        self.max_iter = max_iter

    def forward(self, C):
        """
        C: ä»£ä»·çŸ©é˜µ [B, n, m], æ¯”å¦‚ 1 - cosine_similarity
        """
        B, n, m = C.shape
        device = C.device
        
        # å‡åŒ€è¾¹é™…åˆ†å¸ƒ
        a = torch.ones((B, n), device=device) / n
        b = torch.ones((B, m), device=device) / m
        
        K = torch.exp(-C / self.epsilon)
        u = torch.ones((B, n), device=device) / n
        
        for _ in range(self.max_iter):
            v = b / (torch.bmm(K.transpose(1, 2), u.unsqueeze(-1)).squeeze(-1) + 1e-8)
            u = a / (torch.bmm(K, v.unsqueeze(-1)).squeeze(-1) + 1e-8)
            
        P = u.unsqueeze(-1) * K * v.unsqueeze(1) # [B, n, m]
        ot_loss = torch.sum(P * C, dim=(1, 2))
        return P, ot_loss

# ==========================================
# 3. OT å¼•å¯¼é—¨æŽ§èžåˆ (Gated Fusion)
# ==========================================
class OTGatedFusion(nn.Module):
    def __init__(self, d):
        super().__init__()
        self.gate_v = nn.Sequential(nn.Linear(2*d, d), nn.Sigmoid())
        self.gate_t = nn.Sequential(nn.Linear(2*d, d), nn.Sigmoid())
        
    def forward(self, V, T, P):
        """
        V: [B, n, d], T: [B, m, d], P: [B, n, m]
        """
        # è§†è§‰ä¾§ï¼šé€šè¿‡ P èšåˆæ–‡æœ¬ä¿¡æ¯
        # P: [B, n, m], T: [B, m, d] -> [B, n, d]
        T_aligned = torch.bmm(P, T) 
        g_v = self.gate_v(torch.cat([V, T_aligned], dim=-1))
        V_fused = V + g_v * T_aligned
        
        # æ–‡æœ¬ä¾§ï¼šé€šè¿‡ P^T èšåˆè§†è§‰ä¿¡æ¯
        # P^T: [B, m, n], V: [B, n, d] -> [B, m, d]
        V_aligned = torch.bmm(P.transpose(1, 2), V)
        g_t = self.gate_t(torch.cat([T, V_aligned], dim=-1))
        T_fused = T + g_t * V_aligned
        
        return V_fused, T_fused

# ==========================================
# æ ¸å¿ƒæ¨¡åž‹ï¼šOTLGAModel (Optimal Transport with Local-Global Attention)
# ==========================================
class OTLGAModel(nn.Module):
    def __init__(self, 
                 vit_type='vit_base', 
                 vit_path='', 
                 freeze_vit=True, 
                 freeze_layers=8, # è‹¥éƒ¨åˆ†å†»ç»“ï¼ŒæŒ‡å®šå†»ç»“å‰å‡ å±‚
                 c_embed_dim=256,
                 max_txt_len=128,
                 bert_model='base'):  # æ–°å¢žï¼šæ”¯æŒåŒ»å­¦é¢†åŸŸBERT
        super().__init__()
        
        # 1. è§†è§‰ç¼–ç å™¨ (ViT)
        if vit_type == 'eva_vit':
            self.visual_encoder = create_eva_vit_g(vit_path, 224, precision="fp16")
            vision_dim = 1408
        else:
            self.visual_encoder = get_ViT(vit_path, 224)
            vision_dim = 768
            
        # --- ç­–ç•¥ 1: ViT å†»ç»“ç­–ç•¥ ---
        if freeze_vit:
            for param in self.visual_encoder.parameters():
                param.requires_grad = False
            logging.info("Visual Encoder å…¨å†»ç»“")
        elif freeze_layers > 0:
            # éƒ¨åˆ†å†»ç»“ç¤ºä¾‹ (é’ˆå¯¹ ViT ç»“æž„)
            if hasattr(self.visual_encoder, 'blocks'):
                for i, block in enumerate(self.visual_encoder.blocks):
                    if i < freeze_layers:
                        for param in block.parameters():
                            param.requires_grad = False
                logging.info(f"Visual Encoder å†»ç»“å‰ {freeze_layers} å±‚")

        # 2. æ–‡æœ¬ç¼–ç å™¨ - æ”¯æŒåŒ»å­¦é¢†åŸŸBERT
        from transformers import AutoTokenizer, BertModel
        
        if bert_model == 'clinical':
            model_name = 'emilyalsentzer/Bio_ClinicalBERT'
            print(f"  ðŸ“š ä½¿ç”¨ClinicalBERT (åœ¨MIMIC-IIIä¸Šé¢„è®­ç»ƒ)")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.text_encoder = BertModel.from_pretrained(model_name)
        elif bert_model == 'bio':
            model_name = 'dmis-lab/biobert-v1.1'
            print(f"  ðŸ“š ä½¿ç”¨BioBERT (åœ¨ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸Šé¢„è®­ç»ƒ)")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.text_encoder = BertModel.from_pretrained(model_name)
        else:
            self.text_encoder = BertModel.from_pretrained('bert-base-uncased')
            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        
        text_dim = self.text_encoder.config.hidden_size
        
        # 3. æŠ•å½±å±‚
        self.vision_proj = nn.Linear(vision_dim, c_embed_dim)
        self.text_proj = nn.Linear(text_dim, c_embed_dim)
        
        # 4. åˆ›æ–°æ¨¡å—
        self.lga = LocalGlobalAttention(c_embed_dim)
        self.ot = EntropicOT()
        self.gated_fusion = OTGatedFusion(c_embed_dim)
        
        self.temp = nn.Parameter(torch.ones([]) * 0.5)  # å¢žå¤§åˆå§‹æ¸©åº¦é¿å…ç‰¹å¾å´©æºƒ
        self.max_txt_len = max_txt_len

    def forward(self, image, text_input, is_multiview=False):
        """
        image: [B, 3, H, W] æˆ– [B, V, 3, H, W] (Vä¸ºè§†è§’æ•°)
        text_input: List[str]
        """
        device = image.device
        B = image.size(0)
        
        # --- ç­–ç•¥ 4: å¤šè§†è§’æ”¯æŒ ---
        if is_multiview and image.dim() == 5:
            # ç®€å•ç­–ç•¥ï¼šåˆ†åˆ«é€šè¿‡ç¼–ç å™¨åŽå–å¹³å‡æˆ–æ‹¼æŽ¥
            B, V, C, H, W = image.shape
            image = image.view(-1, C, H, W)
            
        if hasattr(self.visual_encoder, 'image_size'):  # åˆ¤æ–­æ˜¯å¦ä¸º EVA-ViT
            image = image.to(torch.float16)
            v_embeds = self.visual_encoder(image) # [B*V, n, dim]
        else:
            v_embeds = self.visual_encoder(image) # [B*V, n, dim]
        
        if v_embeds.dtype == torch.float16:
            v_embeds = v_embeds.to(torch.float32)
        
        if is_multiview:
            v_embeds = v_embeds.view(B, -1, v_embeds.size(-2), v_embeds.size(-1))
            V_local = v_embeds.mean(dim=1) # [B, n, dim] è·¨è§†è§’å¹³å‡
        else:
            V_local = v_embeds


        # æ˜ å°„ä¸Žå½’ä¸€åŒ–
        V_local = self.vision_proj(V_local)
        v_global = V_local.mean(dim=1)
        
        # æ–‡æœ¬å¤„ç†
        tokens = self.tokenizer(text_input, padding=True, truncation=True, 
                               max_length=self.max_txt_len, return_tensors="pt").to(device)
        T_full = self.text_encoder(**tokens).last_hidden_state
        T_local = self.text_proj(T_full)
        
        # attention_mask: 1 for real tokens, 0 for padding
        attention_mask = tokens['attention_mask'].unsqueeze(-1).float()  # [B, seq_len, 1]
        t_global = (T_local * attention_mask).sum(dim=1) / attention_mask.sum(dim=1).clamp(min=1)  # [B, d]
        
        # LGA å¢žå¼º
        V_local, v_global = self.lga(V_local, v_global)
        T_local, t_global = self.lga(T_local, t_global)
        
        # OT å¯¹é½
        # è®¡ç®—ä»£ä»·çŸ©é˜µ (1 - cosine similarity)
        V_norm = F.normalize(V_local, p=2, dim=-1)
        T_norm = F.normalize(T_local, p=2, dim=-1)
        C = 1.0 - torch.bmm(V_norm, T_norm.transpose(1, 2))
        P, ot_loss = self.ot(C)
        
        # é—¨æŽ§èžåˆ
        V_fused, T_fused = self.gated_fusion(V_local, T_local, P)
        
        v_final = F.normalize(V_fused.mean(dim=1), p=2, dim=-1)
        attention_mask = tokens['attention_mask'].unsqueeze(-1).float()  # [B, seq_len, 1]
        t_final = (T_fused * attention_mask).sum(dim=1) / attention_mask.sum(dim=1).clamp(min=1)  # [B, d]
        t_final = F.normalize(t_final, p=2, dim=-1)
        
        return v_final, t_final, ot_loss, T_fused

